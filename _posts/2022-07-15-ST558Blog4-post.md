ST558 Fourth Blog Post
================
Shan Luo
2022-07-15

# Code for render

``` r
rmarkdown::render("/Users/sl666/Desktop/ST558/ShanLLL52.github.io/_Rmd/2022-07-15-ST558Blog4-post.Rmd", 
          output_format = "github_document", 
          output_dir = "/Users/sl666/Desktop/ST558/ShanLLL52.github.io/_posts",
          output_options = list(
            html_preview = FALSE))
```

I think the most intersting method is random forest.

Random forest follows Bootstrap Aggregation idea. We will create
multiple trees from bootstrap samples and average the results. But, we
will use a random subset of predictors for each bootstrap tree fit
instead of using all predictors. It may make bagged trees predictions
more correlated, which can help with reduction of variation.

# Example

## Data

``` r
heart <- read_csv("/Users/sl666/Desktop/ST558/heart.csv")
```

    ## Rows: 918 Columns: 12
    ## ── Column specification ─────────────────────────────────────────
    ## Delimiter: ","
    ## chr (5): Sex, ChestPainType, RestingECG, ExerciseAngina, ST_S...
    ## dbl (7): Age, RestingBP, Cholesterol, FastingBS, MaxHR, Oldpe...
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
heart$HeartDisease <- as.factor(heart$HeartDisease)
heart$Sex <- as.factor(heart$Sex)
heart <- heart %>% select(HeartDisease, Age, Sex, RestingBP, Cholesterol)
set.seed(1)
trainIndex <- createDataPartition(heart$HeartDisease, p = 0.7, list = FALSE) 
Train <- heart[trainIndex, ]
Test <- heart[-trainIndex, ]
```

## Fit Model

``` r
rffit <- train(HeartDisease ~ ., 
                data = Train, 
                method = "rf", 
                trControl = trainControl(method = "cv",number = 5), 
                preProcess = c("center", "scale"),
               tuneGrid = data.frame(mtry = 1:4))
rffit
```

    ## Random Forest 
    ## 
    ## 643 samples
    ##   4 predictor
    ##   2 classes: '0', '1' 
    ## 
    ## Pre-processing: centered (4), scaled (4) 
    ## Resampling: Cross-Validated (5 fold) 
    ## Summary of sample sizes: 515, 515, 514, 513, 515 
    ## Resampling results across tuning parameters:
    ## 
    ##   mtry  Accuracy   Kappa    
    ##   1     0.6842587  0.3511373
    ##   2     0.6640185  0.3145970
    ##   3     0.6484899  0.2848478
    ##   4     0.6453889  0.2800160
    ## 
    ## Accuracy was used to select the optimal model using the
    ##  largest value.
    ## The final value used for the model was mtry = 1.

``` r
confusionMatrix(data = Test$HeartDisease, 
                reference = predict(rffit, newdata = Test))
```

    ## Confusion Matrix and Statistics
    ## 
    ##           Reference
    ## Prediction   0   1
    ##          0  66  57
    ##          1  21 131
    ##                                           
    ##                Accuracy : 0.7164          
    ##                  95% CI : (0.6591, 0.7689)
    ##     No Information Rate : 0.6836          
    ##     P-Value [Acc > NIR] : 0.1348          
    ##                                           
    ##                   Kappa : 0.4099          
    ##                                           
    ##  Mcnemar's Test P-Value : 7.402e-05       
    ##                                           
    ##             Sensitivity : 0.7586          
    ##             Specificity : 0.6968          
    ##          Pos Pred Value : 0.5366          
    ##          Neg Pred Value : 0.8618          
    ##              Prevalence : 0.3164          
    ##          Detection Rate : 0.2400          
    ##    Detection Prevalence : 0.4473          
    ##       Balanced Accuracy : 0.7277          
    ##                                           
    ##        'Positive' Class : 0               
    ## 
